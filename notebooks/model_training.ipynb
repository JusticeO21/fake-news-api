{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bae8265-fdfa-48da-8293-c1f6f174eb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to stopwords...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer;\n",
    "from sklearn.model_selection import train_test_split;\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "nltk.download(\"punkt\", \"stopwords\")\n",
    "stws = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff29501-beec-4332-9043-b889a2e079aa",
   "metadata": {},
   "source": [
    "### Load dataset which we have already processed in the data analysis stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17341b2c-186a-4824-8397-35369031cea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_lower</th>\n",
       "      <th>fake_or_factual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yearold girl who had apparently given birth sh...</td>\n",
       "      <td>Fake News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>buried in trump s bonkers interview with new y...</td>\n",
       "      <td>Fake News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>women make up over 50 percent of this country ...</td>\n",
       "      <td>Fake News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>backed fighters in syria was being discussed a...</td>\n",
       "      <td>Factual News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sharing government britain s minister for the ...</td>\n",
       "      <td>Factual News</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text_lower fake_or_factual\n",
       "0  yearold girl who had apparently given birth sh...       Fake News\n",
       "1  buried in trump s bonkers interview with new y...       Fake News\n",
       "2  women make up over 50 percent of this country ...       Fake News\n",
       "3  backed fighters in syria was being discussed a...    Factual News\n",
       "4  sharing government britain s minister for the ...    Factual News"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/pre_processed_fake_news.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30758f0-a939-4f94-8e15-e2f266a573eb",
   "metadata": {},
   "source": [
    "## Tokenization and Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43929708-9321-4509-b3ba-84f2c25b1b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenise and remove stopwords from  our data using using the nltk word tokenizer\n",
    "df[\"tokens\"] = df[\"text_lower\"].apply(lambda x: [word for word in word_tokenize(x) if word not in stws])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac8c764-9b74-4ece-9772-2fa2eb1a9a83",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3968fdff-0963-4372-b174-e0953183f1e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_lower</th>\n",
       "      <th>fake_or_factual</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yearold girl who had apparently given birth sh...</td>\n",
       "      <td>Fake News</td>\n",
       "      <td>[yearold, girl, apparently, given, birth, shor...</td>\n",
       "      <td>yearold girl apparently given birth shortly ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>buried in trump s bonkers interview with new y...</td>\n",
       "      <td>Fake News</td>\n",
       "      <td>[buried, trump, bonkers, interview, new, york,...</td>\n",
       "      <td>buried trump bonkers interview new york time r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>women make up over 50 percent of this country ...</td>\n",
       "      <td>Fake News</td>\n",
       "      <td>[women, make, 50, percent, country, grossly, u...</td>\n",
       "      <td>woman make 50 percent country grossly underrep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>backed fighters in syria was being discussed a...</td>\n",
       "      <td>Factual News</td>\n",
       "      <td>[backed, fighters, syria, discussed, highest, ...</td>\n",
       "      <td>backed fighter syria discussed highest level c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sharing government britain s minister for the ...</td>\n",
       "      <td>Factual News</td>\n",
       "      <td>[sharing, government, britain, minister, regio...</td>\n",
       "      <td>sharing government britain minister region sai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text_lower fake_or_factual  \\\n",
       "0  yearold girl who had apparently given birth sh...       Fake News   \n",
       "1  buried in trump s bonkers interview with new y...       Fake News   \n",
       "2  women make up over 50 percent of this country ...       Fake News   \n",
       "3  backed fighters in syria was being discussed a...    Factual News   \n",
       "4  sharing government britain s minister for the ...    Factual News   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [yearold, girl, apparently, given, birth, shor...   \n",
       "1  [buried, trump, bonkers, interview, new, york,...   \n",
       "2  [women, make, 50, percent, country, grossly, u...   \n",
       "3  [backed, fighters, syria, discussed, highest, ...   \n",
       "4  [sharing, government, britain, minister, regio...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  yearold girl apparently given birth shortly ar...  \n",
       "1  buried trump bonkers interview new york time r...  \n",
       "2  woman make 50 percent country grossly underrep...  \n",
       "3  backed fighter syria discussed highest level c...  \n",
       "4  sharing government britain minister region sai...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "# Lemmatize and join words back into sentences\n",
    "df[\"lemmatized\"] = df[\"tokens\"].apply(lambda x: \" \".join([lemmatizer.lemmatize(word) for word in x]))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbbd4f11-53fb-4e13-a4a1-c7a36291c896",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = df[[\"lemmatized\", \"fake_or_factual\"]].copy()\n",
    "training_data.rename(columns = {\"lemmatized\": \"news\", \"fake_or_factual\": \"label\"}, inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0eff39e-f19a-41ea-8bee-ba1bcb06c905",
   "metadata": {},
   "source": [
    "## Classification model training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24b32052-cc2c-415b-8fca-6d6c08efd8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_classifier_pipeline(data, model_class, vectorizer_type=\"count\", pos_data=None, test_size=0.35, random_state=42):\n",
    "    \"\"\"\n",
    "    data: dictionary with keys \"news\" and \"label\" or list of tuples [(label, text), ...]\n",
    "    model_class: a scikit-learn classifier class, e.g., LogisticRegression, GaussianNB\n",
    "    vectorizer_type: \"count\" or \"tfidf\"\n",
    "    pos_data: optional list of POS-tagged features aligned with data[\"data\"]\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(data, list):\n",
    "        y_data, x_data = zip(*data)\n",
    "    else:\n",
    "        x_data = data[\"news\"]\n",
    "        y_data = data[\"label\"]\n",
    "    \n",
    "    x_train_texts, x_test_texts, y_train, y_test = train_test_split(\n",
    "        x_data, y_data, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    if vectorizer_type == \"count\":\n",
    "        vectorizer = CountVectorizer()\n",
    "    elif vectorizer_type == \"tfidf\":\n",
    "        vectorizer = TfidfVectorizer(ngram_range=(1,3), max_features=5000)\n",
    "    else:\n",
    "        raise ValueError(\"vectorizer_type must be 'count' or 'tfidf'\")\n",
    "    \n",
    "    x_train = vectorizer.fit_transform(x_train_texts)\n",
    "    x_test = vectorizer.transform(x_test_texts)\n",
    "    \n",
    "    if pos_data is not None:\n",
    "        pos_train = [pos_data[i] for i in range(len(x_train_texts))]\n",
    "        pos_test = [pos_data[i] for i in range(len(x_train_texts), len(data[\"data\"]))]\n",
    "        \n",
    "        pos_vec_train = vectorizer.fit_transform(pos_train)\n",
    "        pos_vec_test = vectorizer.transform(pos_test)\n",
    "        \n",
    "        x_train = hstack([x_train, pos_vec_train])\n",
    "        x_test = hstack([x_test, pos_vec_test])\n",
    "    \n",
    "    model = model_class()\n",
    "    \n",
    "    # GaussianNB needs dense arrays\n",
    "    if issubclass(model_class, GaussianNB):\n",
    "        x_train = x_train.toarray()\n",
    "        x_test = x_test.toarray()\n",
    "\n",
    "    if issubclass(model_class, LogisticRegression):\n",
    "       model = model_class(max_iter=2000, C=1.0,  class_weight='balanced', solver='liblinear')\n",
    "    \n",
    "    model.fit(x_train, y_train)\n",
    "    predictions = model.predict(x_test)\n",
    "    \n",
    "    acc_score = accuracy_score(y_test, predictions)\n",
    "    print(\"Accuracy score:\", acc_score)\n",
    "    print(classification_report(y_test, predictions))\n",
    "    \n",
    "    return {\"model\": model, \"vectorizer\": vectorizer, \"accuracy_score\": acc_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b6b662e-6a30-40a6-866e-5197be4eb166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.8714285714285714\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Factual News       0.85      0.92      0.88        36\n",
      "   Fake News       0.90      0.82      0.86        34\n",
      "\n",
      "    accuracy                           0.87        70\n",
      "   macro avg       0.87      0.87      0.87        70\n",
      "weighted avg       0.87      0.87      0.87        70\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model1 = custom_classifier_pipeline(training_data, LogisticRegression, \"tfidf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cb17913-64a8-4bc4-8fc6-5c28ce118842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.8142857142857143\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Factual News       0.77      0.92      0.84        36\n",
      "   Fake News       0.89      0.71      0.79        34\n",
      "\n",
      "    accuracy                           0.81        70\n",
      "   macro avg       0.83      0.81      0.81        70\n",
      "weighted avg       0.83      0.81      0.81        70\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model2 = custom_classifier_pipeline(training_data, LogisticRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59cd1a3e-b94d-4f41-8120-250c17d6d718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.8142857142857143\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Factual News       0.85      0.78      0.81        36\n",
      "   Fake News       0.78      0.85      0.82        34\n",
      "\n",
      "    accuracy                           0.81        70\n",
      "   macro avg       0.82      0.82      0.81        70\n",
      "weighted avg       0.82      0.81      0.81        70\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model3 = custom_classifier_pipeline(training_data, GaussianNB, \"tfidf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b9d6d24-e7a8-406e-a1f3-aa8374503748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.7714285714285715\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Factual News       0.78      0.78      0.78        36\n",
      "   Fake News       0.76      0.76      0.76        34\n",
      "\n",
      "    accuracy                           0.77        70\n",
      "   macro avg       0.77      0.77      0.77        70\n",
      "weighted avg       0.77      0.77      0.77        70\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model4 = custom_classifier_pipeline(training_data, GaussianNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64475b0-6dd4-4d12-9911-c280ad5d89d6",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "After evaluating four model configurations combining Logistic Regression and GaussianNB with Count and TF-IDF vectorizers, we found that Logistic Regression paired with TF-IDF (1â€“3 n-grams, max_features=5000) achieved the highest accuracy of 87%. TF-IDF effectively captures contextual patterns in text, giving Logistic Regression an edge over GaussianNB. Overall, Logistic Regression consistently outperformed GaussianNB across both vectorization methods, making it the preferred choice for our fake news detection model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40e1dd1a-3784-4552-a805-e82c7a2393ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['quab_vectorizer.pkl']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model1[\"model\"], \"kbap_model.pkl\")\n",
    "joblib.dump(model1[\"vectorizer\"], \"kbap_vectorizer.pkl\")\n",
    "\n",
    "joblib.dump(model3[\"model\"], \"quab_model.pkl\")\n",
    "joblib.dump(model3[\"vectorizer\"], \"quab_vectorizer.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
